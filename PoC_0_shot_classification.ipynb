{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "IS-ZfwyIVM0K",
        "-T53nEheWGBb",
        "J4vT8Ci-YaIB",
        "wLLab46aXdT3",
        "QkMYkwiqCfm0",
        "X3QHTo0xius7"
      ],
      "authorship_tag": "ABX9TyO8xW7TzpDRhkCKcMSEuudW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/delemarchand2020/TestsAzureAI/blob/main/PoC_0_shot_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Classification avec AzureVision OCR + Embedding Azure AI (OpenAI) + Cos-similarité\n",
        "https://medium.com/@jh.baek.sd/concised-vector-embeddings-for-beginners-guide-3907793e7b10\n",
        "\n",
        "Autre solution avec LLM:\n",
        "* https://levelup.gitconnected.com/advanced-document-classification-with-llms-8801eaee3c58\n",
        "\n",
        "Autre solution avec modèle BERT pré-entrainé et finetuné pour la classification:\n",
        "* https://medium.com/@j622amilah/zero-shot-classification-with-embeddings-1ebc58f48726\n",
        ">* https://huggingface.co/mtheo/camembert-base-xnli\n",
        ">* https://huggingface.co/facebook/bart-large-mnli\n",
        "* https://joeddav.github.io/blog/2020/05/29/ZSL.html\n"
      ],
      "metadata": {
        "id": "ruEs4TP4VVJK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Installation des librairies"
      ],
      "metadata": {
        "id": "IS-ZfwyIVM0K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PBFSnNQ3VC2K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcc379af-a4e5-4111-d7dc-a4ddeb4b6f52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m401.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.3/194.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q azure-ai-vision-imageanalysis"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "kWhksfhTVhpR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31b21827-49c2-4f8d-9aae-19088e90df02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.2/318.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pdf2image"
      ],
      "metadata": {
        "id": "j74wbwgeCz0U"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils"
      ],
      "metadata": {
        "id": "jvpk2OjUEvdi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "175bfd30-8daf-487b-b712-1931268e2b0a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 45 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 696 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.4 [186 kB]\n",
            "Fetched 186 kB in 0s (452 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 121926 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.4_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.4) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q docx2pdf"
      ],
      "metadata": {
        "id": "pW95DGTaG5Se"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai num2words matplotlib plotly scipy scikit-learn pandas tiktoken"
      ],
      "metadata": {
        "id": "SYyDVSrKekOz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b897c9e-4c14-4109-f1bb-4a8b183b0c1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chargement des clés Azure et OpenAI"
      ],
      "metadata": {
        "id": "-T53nEheWGBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VISION_ENDPOINT = \"https://demo-desj-azure-ai.cognitiveservices.azure.com/\""
      ],
      "metadata": {
        "id": "YFTSE7XuWvIT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cljp5xtdO1-7",
        "outputId": "dd22350f-7d85-4067-91f6-fa03d1c98322"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin du fichier après le montage de Google Drive\n",
        "file_path = \"/content/gdrive/MyDrive/key.openai.txt\"\n",
        "\n",
        "# Lire le contenu du fichier\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Configuration initiale de l'API OpenAI\n",
        "openai_api_key = content"
      ],
      "metadata": {
        "id": "EsJbzweyLqHR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin du fichier après le montage de Google Drive\n",
        "file_path = \"/content/gdrive/MyDrive/key.azure.txt\"\n",
        "\n",
        "# Lire le contenu du fichier\n",
        "with open(file_path, 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Configuration initiale de l'API Azure\n",
        "VISION_KEY = content"
      ],
      "metadata": {
        "id": "8M4TpXN1O-78"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chargement d'images de test"
      ],
      "metadata": {
        "id": "J4vT8Ci-YaIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/recu_masso_photo.png -O recu_masso_photo.png\n",
        "!wget https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/billet_medecin.png -O billet_medecin.png\n",
        "!wget https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/fac_ortho.jpg -O fac_ortho.jpg\n",
        "!wget https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/releve_notes.pdf -O releve_notes.pdf\n",
        "!wget https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/McGill_Enrolment_Letter_2023-2024.pdf -O McGill_Enrolment_Letter_2023-2024.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06mLBLwOM8OE",
        "outputId": "0aec3e95-4a52-457c-af2e-30829d4a308c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-11 11:04:12--  https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/recu_masso_photo.png\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/recu_masso_photo.png [following]\n",
            "--2024-07-11 11:04:12--  https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/recu_masso_photo.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3541129 (3.4M) [image/png]\n",
            "Saving to: ‘recu_masso_photo.png’\n",
            "\n",
            "recu_masso_photo.pn 100%[===================>]   3.38M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-07-11 11:04:13 (41.0 MB/s) - ‘recu_masso_photo.png’ saved [3541129/3541129]\n",
            "\n",
            "--2024-07-11 11:04:13--  https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/billet_medecin.png\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/billet_medecin.png [following]\n",
            "--2024-07-11 11:04:13--  https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/billet_medecin.png\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8827101 (8.4M) [application/octet-stream]\n",
            "Saving to: ‘billet_medecin.png’\n",
            "\n",
            "billet_medecin.png  100%[===================>]   8.42M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2024-07-11 11:04:14 (59.1 MB/s) - ‘billet_medecin.png’ saved [8827101/8827101]\n",
            "\n",
            "--2024-07-11 11:04:14--  https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/fac_ortho.jpg\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/fac_ortho.jpg [following]\n",
            "--2024-07-11 11:04:14--  https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/fac_ortho.jpg\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 601616 (588K) [image/jpeg]\n",
            "Saving to: ‘fac_ortho.jpg’\n",
            "\n",
            "fac_ortho.jpg       100%[===================>] 587.52K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-07-11 11:04:14 (12.5 MB/s) - ‘fac_ortho.jpg’ saved [601616/601616]\n",
            "\n",
            "--2024-07-11 11:04:14--  https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/releve_notes.pdf\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/releve_notes.pdf [following]\n",
            "--2024-07-11 11:04:15--  https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/releve_notes.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23511 (23K) [application/octet-stream]\n",
            "Saving to: ‘releve_notes.pdf’\n",
            "\n",
            "releve_notes.pdf    100%[===================>]  22.96K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2024-07-11 11:04:15 (8.52 MB/s) - ‘releve_notes.pdf’ saved [23511/23511]\n",
            "\n",
            "--2024-07-11 11:04:15--  https://github.com/delemarchand2020/ExtractDataFromImageDoc/raw/main/McGill_Enrolment_Letter_2023-2024.pdf\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/McGill_Enrolment_Letter_2023-2024.pdf [following]\n",
            "--2024-07-11 11:04:15--  https://raw.githubusercontent.com/delemarchand2020/ExtractDataFromImageDoc/main/McGill_Enrolment_Letter_2023-2024.pdf\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 154612 (151K) [application/octet-stream]\n",
            "Saving to: ‘McGill_Enrolment_Letter_2023-2024.pdf’\n",
            "\n",
            "McGill_Enrolment_Le 100%[===================>] 150.99K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-07-11 11:04:15 (4.76 MB/s) - ‘McGill_Enrolment_Letter_2023-2024.pdf’ saved [154612/154612]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fonctions utiles"
      ],
      "metadata": {
        "id": "mv6gBjyOXco_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Conversion PDF ou DOCX en image"
      ],
      "metadata": {
        "id": "wLLab46aXdT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pdf2image import convert_from_path\n",
        "import os\n",
        "\n",
        "def convert_pdf_to_jpg(file_path):\n",
        "    # Vérifier si le fichier existe\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(\"Le fichier spécifié n'existe pas.\")\n",
        "\n",
        "    # Vérifier si le fichier est un fichier PDF\n",
        "    if not file_path.lower().endswith('.pdf'):\n",
        "        return file_path\n",
        "\n",
        "    # Chemin du fichier JPEG de sortie\n",
        "    jpg_path = os.path.splitext(file_path)[0] + \".jpg\"\n",
        "\n",
        "    # Convertir le PDF en une liste d'images PIL\n",
        "    images = convert_from_path(file_path)\n",
        "\n",
        "    # Sauvegarder la première image en tant que fichier JPEG\n",
        "    images[0].save(jpg_path, \"JPEG\")\n",
        "\n",
        "    # Retourner le chemin du fichier JPEG\n",
        "    return jpg_path"
      ],
      "metadata": {
        "id": "T2pZejvGCqTV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx2pdf import convert\n",
        "\n",
        "def convert_docx_to_pdf(file_path):\n",
        "    # Vérifier si le fichier existe\n",
        "    if not os.path.isfile(file_path):\n",
        "        raise FileNotFoundError(\"Le fichier spécifié n'existe pas.\")\n",
        "\n",
        "    # Vérifier si le fichier est un fichier DOCX\n",
        "    if not file_path.lower().endswith('.docx'):\n",
        "        return file_path\n",
        "\n",
        "    # Chemin du fichier PDF de sortie\n",
        "    pdf_path = os.path.splitext(file_path)[0] + \".pdf\"\n",
        "\n",
        "    # Convertir le fichier DOCX en PDF\n",
        "    convert(file_path, pdf_path)\n",
        "\n",
        "    # Retourner le chemin du fichier PDF\n",
        "    return convert_pdf_to_jpg(pdf_path)\n"
      ],
      "metadata": {
        "id": "Xm3P4udJGmtc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def get_PIL_image(img_path):\n",
        "  with open(img_path, 'rb') as image_file:\n",
        "      content = image_file.read()\n",
        "  return Image.open(io.BytesIO(content))"
      ],
      "metadata": {
        "id": "xPgE6VUCMnRk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def convert_cos_sim_to_angle(cos_sim):\n",
        "    # Calcul de l'angle en radians\n",
        "    angle_rad = np.arccos(cos_sim)\n",
        "\n",
        "    # Conversion de radians en degrés\n",
        "    angle_deg = np.degrees(angle_rad)\n",
        "\n",
        "    return round(angle_deg,1)"
      ],
      "metadata": {
        "id": "tkqwqsMaKsYs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Pre-traitement textes"
      ],
      "metadata": {
        "id": "QkMYkwiqCfm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Définir les expressions régulières pour les dates, les prix, les mots français (y compris les accents) et les nombres\n",
        "date_regex = re.compile(r'\\b\\d{1,2}/\\d{1,2}/\\d{2,4}\\b|\\b\\d{1,2}-\\d{1,2}-\\d{2,4}\\b|\\b\\d{4}-\\d{2}-\\d{2}\\b|\\b\\d{2,4}/\\d{2}/\\d{2,4}\\b')\n",
        "price_regex = re.compile(r'\\b\\d+[\\.,]?\\d*\\s?(€|EUR|euros?|dollars?|USD|CHF)\\b')\n",
        "french_word_regex = re.compile(r'\\b[a-zA-ZàâäéèêëîïôöùûüÿçÀÂÄÉÈÊËÎÏÔÖÙÛÜŸÇ]+\\b')\n",
        "number_regex = re.compile(r'\\b\\d+\\b')\n",
        "\n",
        "# Définir une expression régulière pour les éléments indésirables (ponctuation et autres caractères non désirés)\n",
        "unwanted_regex = re.compile(r'[^\\w\\s€|EUR|euros|dollars|USD|CHFàâäéèêëîïôöùûüÿçÀÂÄÉÈÊËÎÏÔÖÙÛÜŸÇ\\'/-]')\n",
        "\n",
        "# Fonction pour filtrer le texte\n",
        "def filter_text(text):\n",
        "    filtered_elements = []\n",
        "    words = text.split()\n",
        "\n",
        "    for word in words:\n",
        "        # Supprimer les signes de ponctuation et autres caractères non désirés\n",
        "        clean_word = unwanted_regex.sub('', word)\n",
        "\n",
        "        if date_regex.match(clean_word) or price_regex.match(clean_word) or french_word_regex.match(clean_word) or number_regex.match(clean_word):\n",
        "            filtered_elements.append(clean_word)\n",
        "\n",
        "    return ' '.join(filtered_elements)"
      ],
      "metadata": {
        "id": "zW2Hg2sjCs61"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test pré-traitement\n",
        "# Exemple d'utilisation\n",
        "text = \"Bonjour, je veux acheter une voiture le 12/06/2024 pour 1500 EUR. Mon chat aime bien jouer avec la souris. La date d'aujourd'hui est 2024-06-10. J'ai 2 chats et 1 chien.\"\n",
        "filtered_text = filter_text(text)\n",
        "print(filtered_text)"
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7l2jja9MXuS",
        "outputId": "d5f3e6e8-cedb-485d-afcb-23396a3ea615"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bonjour je veux acheter une voiture le 12/06/2024 pour 1500 EUR Mon chat aime bien jouer avec la souris La date d'aujourd'hui est 2024-06-10 J'ai 2 chats et 1 chien\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Appels Azure Vision\n",
        "https://learn.microsoft.com/fr-ca/python/api/overview/azure/ai-vision-imageanalysis-readme?view=azure-python-preview"
      ],
      "metadata": {
        "id": "0BQUwvcHXm8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
        "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "\n",
        "# Set the values of your computer vision endpoint and computer vision key\n",
        "# as environment variables:\n",
        "try:\n",
        "    endpoint = VISION_ENDPOINT\n",
        "    key = VISION_KEY\n",
        "except KeyError:\n",
        "    print(\"Missing environment variable 'VISION_ENDPOINT' or 'VISION_KEY'\")\n",
        "    print(\"Set them before running this sample.\")\n",
        "    exit()\n",
        "\n",
        "# Create an Image Analysis client for synchronous operations\n",
        "client = ImageAnalysisClient(\n",
        "    endpoint=endpoint,\n",
        "    credential=AzureKeyCredential(key)\n",
        ")"
      ],
      "metadata": {
        "id": "mdBVcijLXg7i"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test AzureVision\n",
        "test=True\n",
        "\n",
        "if test:\n",
        "    # Load image to analyze into a 'bytes' object\n",
        "    with open(\"billet_medecin.png\", \"rb\") as f:\n",
        "        image_data = f.read()\n",
        "\n",
        "    # Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
        "    result = client.analyze(\n",
        "        image_data=image_data,\n",
        "        visual_features=[VisualFeatures.READ, VisualFeatures.OBJECTS, VisualFeatures.TAGS]\n",
        "    )\n",
        "\n",
        "    # Print text (OCR) analysis results to the console\n",
        "    #print(\"Image analysis results:\")\n",
        "    #print(\" Read:\")\n",
        "    #if result.read is not None:\n",
        "    #  if result.read.blocks != []:\n",
        "    #    for line in result.read.blocks[0].lines:\n",
        "    #        print(f\"   Line: '{line.text}', Bounding box {line.bounding_polygon}\")\n",
        "    #        for word in line.words:\n",
        "    #            print(f\"     Word: '{word.text}', Bounding polygon {word.bounding_polygon}, Confidence {word.confidence:.4f}\")\n",
        "    if result.tags is not None:\n",
        "      print(\" Tags:\")\n",
        "      for o in result.tags.list:\n",
        "          print(f\"   tag: '{o}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cVSkc4-cX5g",
        "outputId": "4bebee5a-2798-48fb-e80d-7525c2e294b3",
        "cellView": "form"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Tags:\n",
            "   tag: '{'name': 'text', 'confidence': 0.9999969005584717}'\n",
            "   tag: '{'name': 'handwriting', 'confidence': 0.9552189111709595}'\n",
            "   tag: '{'name': 'paper', 'confidence': 0.921732485294342}'\n",
            "   tag: '{'name': 'ink', 'confidence': 0.8959401249885559}'\n",
            "   tag: '{'name': 'paper product', 'confidence': 0.8882706165313721}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get text (OCR) analysis results\n",
        "def get_all_words(result):\n",
        "    all_words = []\n",
        "    if result.read.blocks != []:\n",
        "      for line in result.read.blocks[0].lines:\n",
        "          for word in line.words:\n",
        "              all_words.append(word.text)\n",
        "    return all_words\n",
        "\n",
        "def get_all_tags(result):\n",
        "    all_tags = []\n",
        "    if result.tags is not None:\n",
        "      all_tags = result.tags.list\n",
        "    return all_tags\n",
        "\n",
        "if test:\n",
        "  print(get_all_words(result))\n",
        "  print(get_all_tags(result))"
      ],
      "metadata": {
        "id": "EnXYFpLWc7xJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cffcdb70-6c83-48f0-c4b7-aac8c457283b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HYGEA', 'clinique', 'médicale', '5100', 'boul.', 'de', 'Maisonneuve', 'O.', 'Bureau', '710', 'Tél.', ':', '514-938-0995', 'Montréal', '(Qc)', 'H4A', '3T2', 'Fax', ':', '514-938-4476', '.Sul,', '31/23', 'Date', ':', 'Marina', 'Grandin', 'Nom', ':', 'Addresse', ':', 'R', 'Custom', 'orthotics', 'Dx', 'Hallux', 'Valgus', 'Geld', 'sman', '87582']\n",
            "[{'name': 'text', 'confidence': 0.9999969005584717}, {'name': 'handwriting', 'confidence': 0.9552189111709595}, {'name': 'paper', 'confidence': 0.921732485294342}, {'name': 'ink', 'confidence': 0.8959401249885559}, {'name': 'paper product', 'confidence': 0.8882706165313721}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty dictionary to store the extracted text\n",
        "text_dict = {}\n",
        "tags_dict = {}\n",
        "\n",
        "def document_texts_detection(img_path, debug=False):\n",
        "    # Check if the image has already been seen\n",
        "    if img_path not in text_dict:\n",
        "        # Load image to analyze into a 'bytes' object\n",
        "        with open(img_path, \"rb\") as f:\n",
        "            image_data = f.read()\n",
        "\n",
        "        # Extract text (OCR) from an image stream. This will be a synchronously (blocking) call.\n",
        "        result = client.analyze(\n",
        "            image_data=image_data,\n",
        "            visual_features=[VisualFeatures.READ, VisualFeatures.TAGS]\n",
        "        )\n",
        "        # Store the extracted text in the dictionary\n",
        "        text_dict[img_path] = get_all_words(result)\n",
        "        tags_dict[img_path] = get_all_tags(result)\n",
        "\n",
        "    # Return the extracted text from the dictionary\n",
        "    return text_dict[img_path], tags_dict[img_path]\n"
      ],
      "metadata": {
        "id": "TQyQ0-cUMQTW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Appels OpenAI pour embeddings\n",
        "https://learn.microsoft.com/en-us/azure/ai-services/openai/tutorials/embeddings?tabs=python-new%2Ccommand-line&pivots=programming-language-python\n",
        "\n",
        "* Voir aussi les compétiteurs : https://blog.voyageai.com/2024/06/10/voyage-multilingual-2-multilingual-embedding-model/\n",
        ">* A essayer : https://docs.voyageai.com/docs/api-key-and-installation"
      ],
      "metadata": {
        "id": "nRloA_YvXvS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "import openai\n",
        "\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "client_openai = OpenAI(api_key=openai.api_key)"
      ],
      "metadata": {
        "id": "B0ESjaQLoFuF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# Initialize an empty dictionary to store the embeddings\n",
        "embeddings_dict = {}\n",
        "\n",
        "def get_text_embedding(input, model=\"text-embedding-3-small\"):\n",
        "\n",
        "    if input != None and input != \"\":\n",
        "        # Check if the input has already been seen\n",
        "        if input not in embeddings_dict:\n",
        "            embeddings_batch_response_openAI = client_openai.embeddings.create(input = [input], model=model)\n",
        "            # Store the embedding in the dictionary\n",
        "            embeddings_dict[input] = np.array([embeddings_batch_response_openAI.data[0].embedding])\n",
        "        # Return the embedding from the dictionary\n",
        "        return embeddings_dict[input]\n",
        "    else:\n",
        "        return np.zeros((1, 1536))"
      ],
      "metadata": {
        "id": "cu7S2WEyHcou"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tests des appels"
      ],
      "metadata": {
        "id": "X3QHTo0xius7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = document_texts_detection(img_path=\"recu_masso_photo.png\", debug=False)"
      ],
      "metadata": {
        "id": "c97yS-pGizik"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Nombre de mots/token détectés: {len(texts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMh0FP2yjjRA",
        "outputId": "87838e14-b1c3-4e94-b83e-af6d4163c901"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre de mots/token détectés: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Texte pour test\n",
        "texte = \"\"\"\n",
        "Université\n",
        "de Montréal\n",
        "Denis Lemarchand\n",
        "4958 avenue grosvenor\n",
        "montreal QC H3W 2M1\n",
        "Canada\n",
        "Programme d'études:\n",
        "Spécialisation:\n",
        "Relevé de notes non officiel\n",
        "Relevé des études de cycles supérieurs\n",
        "Automne 2022\n",
        "217510 Informatique (Maîtrise)\n",
        "Apprentissage automatique (Spécialisation)\n",
        "Date d'émission:\n",
        "Matricule:\n",
        "CPER QC:\n",
        "2024-01-25\n",
        "20241211\n",
        "LEMD02036912\n",
        "Cours\n",
        "IFT 6390\n",
        "Description\n",
        "Crédits:\n",
        "Apprent. machine: fondements\n",
        "Suivis\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Note Points\n",
        "A\n",
        "16.000\n",
        "Moy.gr.\n",
        "2-175-1-0 72 Cours à option\n",
        "Crédits:\n",
        "Suivis\n",
        "Moyenne générale trimestrielle\n",
        "4.000 Total du trimestre\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Moy.\n",
        "4.000\n",
        "Points\n",
        "16.000\n",
        "Hiver 2023\n",
        "Programme d'études:\n",
        "Spécialisation:\n",
        "217510 Informatique (Maîtrise)\n",
        "Apprentissage automatique (Spécialisation)\n",
        "Cours\n",
        "Description\n",
        "Crédits:\n",
        "IFT 6135\n",
        "Aprentissage de représent.\n",
        "Suivis\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Note Points\n",
        "A-\n",
        "14.800\n",
        "Moy.gr.\n",
        "2-175-1-0 72 Cours à option\n",
        "Crédits:\n",
        "Moyenne générale trimestrielle\n",
        "3.700 Total du trimestre\n",
        "Suivis\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Moy.\n",
        "4.000\n",
        "Points\n",
        "14.800\n",
        "Automne 2023\n",
        "Programme d'études:\n",
        "Spécialisation:\n",
        "217510 Informatique (Maîtrise)\n",
        "Apprentissage automatique (Spécialisation)\n",
        "Cours\n",
        "Description\n",
        "Crédits:\n",
        "IFT 6758\n",
        "Science des données\n",
        "Suivis\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Note Points\n",
        "A\n",
        "16.000\n",
        "Moy.gr.\n",
        "2-175-1-0 72 Cours à option\n",
        "Crédits:\n",
        "Suivis\n",
        "Moyenne générale trimestrielle\n",
        "4.000 Total du trimestre\n",
        "4.000\n",
        "Obtenus\n",
        "4.000\n",
        "Moy.\n",
        "4.000\n",
        "Points\n",
        "16.000\n",
        "Hiver 2024\n",
        "Programme d'études:\n",
        "Spécialisation:\n",
        "217510 Informatique (Maîtrise)\n",
        "Intelligence artificielle (Spécialisation)\n",
        "Cours\n",
        "Description\n",
        "Crédits:\n",
        "IFT 6261\n",
        "Traitement des connaissances\n",
        "2-175-1-0 72 Cours à option\n",
        "Suivis\n",
        "Obtenus Note\n",
        "Points\n",
        "Moy.gr.\n",
        "Pour être officiel, le relevé de notes doit porter le sceau de l'Université et la signature du Registraire.\n",
        "Page 1 de 2\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pvBn5arra0QD",
        "cellView": "form"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tests similarité"
      ],
      "metadata": {
        "id": "FXQzYZOJGj1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texte1 = \"recevoir une paye pour son métier\"\n",
        "texte2 = \"une bulletin de salaire emploi ou travail ou profession ou tâche\"\n",
        "vector1 = get_text_embedding(texte1)\n",
        "vector2 = get_text_embedding(texte2)\n",
        "cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfSDYrFPGqXV",
        "outputId": "08cb9691-267a-4fa5-cf9d-271f720e0804"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.51359817]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texte1 = \"recevoir une peine pour son crime\"\n",
        "texte2 = \"une bulletin de salaire emploi ou travail ou profession ou tâche\"\n",
        "vector1 = get_text_embedding(texte1)\n",
        "vector2 = get_text_embedding(texte2)\n",
        "cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vv5SXRHEqQzl",
        "outputId": "e774ef52-7298-401c-d7e4-91296df162f9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.21518421]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texte1 = \"recevoir une peine pour un crime\"\n",
        "texte2 = \"application d'une décision de jugement pénal\"\n",
        "vector1 = get_text_embedding(texte1)\n",
        "vector2 = get_text_embedding(texte2)\n",
        "cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3lPS2E5qYIQ",
        "outputId": "35df4df9-4b50-491b-81cf-d0b643fa2b57"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.56626253]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texte1 = \"recevoir une paye pour son métier\"\n",
        "texte2 = \"application d'une décision de jugement pénal\"\n",
        "vector1 = get_text_embedding(texte1)\n",
        "vector2 = get_text_embedding(texte2)\n",
        "cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rti8ZQ8l_USK",
        "outputId": "f6871aa3-6a9e-484c-ed6a-da215449cd92"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29381242]])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_cosine_similarity(vector1, vector2):\n",
        "    #Produit scalaire des vecteurs\n",
        "    scalar_product = np.dot(vector1, vector2.T)\n",
        "    #Norme euclidienne des vecteurs\n",
        "    norm_vector1 = np.linalg.norm(vector1)\n",
        "    norm_vector2 = np.linalg.norm(vector2)\n",
        "    #Expression analytique du cosinus dans un espace euclidien\n",
        "    cosine = scalar_product / (norm_vector1 * norm_vector2)\n",
        "    return cosine\n",
        "\n",
        "my_cosine_similarity(vector1, vector2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeeVZX37Bw0J",
        "outputId": "bd7a3706-1fc3-404b-fa5b-d1f17417a06a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29381242]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#text-embedding-ada-002, text-embedding-3-small, text-embedding-3-large\n",
        "model=\"text-embedding-3-large\"\n",
        "cosine_similarity(get_text_embedding(texte1, model=model), get_text_embedding(texte2, model=model))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYncEhYyKNDU",
        "outputId": "970c3eac-098a-433f-feca-be61389b3e9b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.29381242]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Classification"
      ],
      "metadata": {
        "id": "AkTfVi4OC4vW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the mapping of sub-types to types\n",
        "type_mapping = {\n",
        "    \"Relevé de notes officiel universitaire\": \"Bulletin de notes\",\n",
        "    \"Relevé de notes non officiel universitaire\": \"Bulletin de notes\",\n",
        "    \"Bulletin cumulatif de notes\": \"Bulletin de notes\",\n",
        "    \"Cours avec moyenne générale trimestrielle\": \"Bulletin de notes\",\n",
        "    \"Crédits obtenus et notes\": \"Bulletin de notes\",\n",
        "\n",
        "    \"Facture session universitaire\": \"Preuve de scolarité\",\n",
        "    \"Horaire de cours\": \"Preuve de scolarité\",\n",
        "    \"Confirmation d'inscription\": \"Preuve de scolarité\",\n",
        "    \"Statut étudiant temps plein ou temps partiel\": \"Preuve de scolarité\",\n",
        "    \"Attestation d'études\": \"Preuve de scolarité\",\n",
        "\n",
        "    \"Certificat médical\": \"Billet du médecin\",\n",
        "    \"Formulaire d'arrêt de travail\": \"Billet du médecin\",\n",
        "    \"Billet du medecin\": \"Billet du médecin\",\n",
        "    \"Prescription médicale\": \"Billet du médecin\",\n",
        "    \"Ordonnance médicale clinique\": \"Billet du médecin\",\n",
        "\n",
        "    \"Spécimen de chèque\": \"Spécimen de chèque\",\n",
        "    \"Payer à l'ordre de\": \"Spécimen de chèque\",\n",
        "    \"100 dollars\": \"Spécimen de chèque\",\n",
        "    \"Banque\": \"Spécimen de chèque\",\n",
        "    \"MICR code\": \"Spécimen de chèque\"#,\n",
        "\n",
        "    #\"Facture soins dentaires\": \"Facture\",\n",
        "    #\"Facture soin thérapeutique\": \"Facture\",\n",
        "    #\"Facture ou reçu soin massage thérapeutique\": \"Facture\",\n",
        "    #\"Facture et services\": \"Facture\"\n",
        "}\n",
        "\n",
        "# Derive types_doc from type_mapping\n",
        "sous_types_doc = list(type_mapping.keys())\n",
        "sous_types_doc_original = sous_types_doc.copy()\n",
        "type_mapping_original = type_mapping.copy()"
      ],
      "metadata": {
        "id": "x31_1u0HJlUZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def classify_text(texte, sous_types_doc, seuil=0.4):\n",
        "    classification = {}\n",
        "    insert_autre = False\n",
        "    total_score = 0\n",
        "\n",
        "    # Calcul de la similarité pour chaque type de document\n",
        "    for sous_type in sous_types_doc:\n",
        "        similarity_score = cosine_similarity(get_text_embedding(texte), get_text_embedding(sous_type))[0][0]\n",
        "        if similarity_score >= seuil:\n",
        "            doc_type = type_mapping[sous_type]\n",
        "            if doc_type in classification:\n",
        "                classification[doc_type][\"cumul des scores de similarité\"] += similarity_score\n",
        "            else:\n",
        "                classification[doc_type] = {\"cumul des scores de similarité\": similarity_score}\n",
        "            total_score += similarity_score\n",
        "        elif not insert_autre:\n",
        "            if \"Autre\" in classification:\n",
        "                classification[\"Autre\"][\"cumul des scores de similarité\"] += seuil\n",
        "            else:\n",
        "                classification[\"Autre\"] = {\"cumul des scores de similarité\": seuil}\n",
        "            total_score += seuil\n",
        "            insert_autre = True\n",
        "\n",
        "    # Calcul du pourcentage de chaque score de similarité\n",
        "    for doc_type, item in classification.items():\n",
        "        item[\"part dans la classification (%)\"] = round((item[\"cumul des scores de similarité\"] / total_score) * 100, 2)\n",
        "\n",
        "    # Tri des résultats par score de similarité en ordre décroissant\n",
        "    sorted_classification = sorted(classification.items(), key=lambda x: x[1][\"cumul des scores de similarité\"], reverse=True)\n",
        "\n",
        "    # Conversion de la liste triée en format JSON\n",
        "    return json.dumps(sorted_classification, indent=4), sorted_classification[0][1]['part dans la classification (%)']"
      ],
      "metadata": {
        "id": "Yfm4kyoxJ2Vt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test classification\n",
        "img_path = convert_pdf_to_jpg(\"Collège des médecins du Québec - Carte de résidence.jpg\") #\"McGill_Enrolment_Letter_2023-2024.pdf\" \"releve_notes.pdf\"\n",
        "img_path = convert_docx_to_pdf(img_path)\n",
        "texte, _ = document_texts_detection(img_path=img_path)\n",
        "texte = \" \".join(texte)\n",
        "classification_sorted, best = classify_text(filter_text(texte), sous_types_doc, seuil=0.29)\n",
        "json.loads(classification_sorted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ9Fj4i4rQj4",
        "outputId": "3c039bb8-274e-4ec0-c2be-0ede9706f065"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Billet du médecin',\n",
              "  {'cumul des scores de similarité': 1.8731655811367012,\n",
              "   'part dans la classification (%)': 33.03}],\n",
              " ['Preuve de scolarité',\n",
              "  {'cumul des scores de similarité': 1.767367786688316,\n",
              "   'part dans la classification (%)': 31.17}],\n",
              " ['Bulletin de notes',\n",
              "  {'cumul des scores de similarité': 1.7404117993434773,\n",
              "   'part dans la classification (%)': 30.69}],\n",
              " ['Autre',\n",
              "  {'cumul des scores de similarité': 0.29,\n",
              "   'part dans la classification (%)': 5.11}]]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test classification avec camembert-base-xnli\n",
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/mtheo/camembert-base-xnli\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "    \"inputs\": f\"{texte}\",\n",
        "    \"parameters\": {\"candidate_labels\": [\"Preuve de scolarité\", \"Billet du médecin\", \"Bulletin de notes\", \"Specimen de chèque\", \"autre\"]},\n",
        "})\n",
        "\n",
        "print(output['labels'])\n",
        "print(output['scores'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZo-ddDtcbqJ",
        "outputId": "3b821b93-9cca-4cd8-ac07-c2adb0108d8e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Preuve de scolarité', 'Billet du médecin', 'Bulletin de notes', 'Specimen de chèque', 'autre']\n",
            "[0.3488912880420685, 0.30435124039649963, 0.13877806067466736, 0.1091211587190628, 0.0988583043217659]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def objective_function(seuil, filtered_text, sous_types_doc):\n",
        "    _, best = classify_text(filtered_text, sous_types_doc, seuil)\n",
        "    return best\n",
        "\n",
        "def find_best_seuil(texte, sous_types_doc, seuil_max=0.45, tol=1e-3):\n",
        "    filtered_text = filter_text(texte)\n",
        "\n",
        "    low = 0.0\n",
        "    high = seuil_max\n",
        "\n",
        "    best_seuil = (low + high) / 2.0\n",
        "    best_value = objective_function(best_seuil, filtered_text, sous_types_doc)\n",
        "\n",
        "    while (high - low) > tol:\n",
        "        mid1 = low + (high - low) / 3\n",
        "        mid2 = high - (high - low) / 3\n",
        "\n",
        "        value1 = objective_function(mid1, filtered_text, sous_types_doc)\n",
        "        value2 = objective_function(mid2, filtered_text, sous_types_doc)\n",
        "\n",
        "        if value1 > value2:\n",
        "            high = mid2\n",
        "        else:\n",
        "            low = mid1\n",
        "\n",
        "        best_seuil = (low + high) / 2.0\n",
        "        best_value = objective_function(best_seuil, filtered_text, sous_types_doc)\n",
        "\n",
        "    return best_seuil, best_value\n",
        "\n",
        "best_seuil, best_value = find_best_seuil(texte, sous_types_doc, seuil_max=0.6)\n",
        "print(f\"Le meilleur seuil est {best_seuil} avec une valeur de {best_value}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inve7R-TYgTt",
        "outputId": "b88c19ff-33dc-4dbe-ee57-11bc118ce1d4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le meilleur seuil est 0.5995432683478957 avec une valeur de 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interface UI pour les tests PO-AA"
      ],
      "metadata": {
        "id": "stynIthZjKca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(img_path, new_type_doc, categorie, parametres, seuil):\n",
        "    global sous_types_doc_original, type_mapping_original, sous_types_doc, type_mapping\n",
        "\n",
        "    img_path = convert_pdf_to_jpg(img_path)\n",
        "    img_path = convert_docx_to_pdf(img_path)\n",
        "    texts, tags = document_texts_detection(img_path=img_path)\n",
        "    texts = \" \".join(texts)\n",
        "    image = get_PIL_image(img_path=img_path)\n",
        "\n",
        "    if \"Réinitialiser la liste des sous-types/types\" in parametres:\n",
        "        types_doc_etendu = sous_types_doc_original.copy()\n",
        "        sous_types_doc = sous_types_doc_original.copy()\n",
        "        type_mapping = type_mapping_original.copy()\n",
        "    else:\n",
        "        types_doc_etendu = sous_types_doc.copy()\n",
        "        if new_type_doc != \"\":\n",
        "            if new_type_doc not in sous_types_doc:\n",
        "                types_doc_etendu.append(new_type_doc)\n",
        "                # Add the new type to the type_mapping dictionary\n",
        "                type_mapping[new_type_doc] = categorie\n",
        "\n",
        "    if \"Nettoyage Texte\" in parametres:\n",
        "        texts = filter_text(texts)\n",
        "\n",
        "    if \"Calcul meilleur seuil\" in parametres:\n",
        "        best_seuil, best = find_best_seuil(texts, types_doc_etendu, seuil_max=float(seuil), tol=1e-3)\n",
        "        seuil = best_seuil\n",
        "\n",
        "    analyse, best = classify_text(texts, types_doc_etendu, seuil=seuil)\n",
        "\n",
        "    return image, seuil, analyse, texts, tags, type_mapping"
      ],
      "metadata": {
        "id": "Z8JJU2p7KTTN"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# URL de l'image du diagramme\n",
        "#diagram_url = \"https://diagrams.helpful.dev/d/d:P8TaRIVr-png-base-64-for-mobile\"\n",
        "diagram_url = \"https://mermaid.ink/img/pako:eNp9U0FOwzAQ_MrKF0CiPCCHSqj0gIRUiSJOuSz2ElZK7GBvKgriQX1HP8Y6TUtpgFxie2cmuzPxh7HBkSlMoteOvKUbxipiU3rQp8UobLlFL3ATbNeQLjDB7AVjRf3OdeCGyphz22BFPSH4FcXEwQN54Hw8Ri9m9xk7f5OIVjJWxYXehOD8-r2LBI-cJS7G1HnzRM6xr3qB1KIlWJGVKzhftOSvb78Rv7BnIbGnJTdcY2RZ7zpuFIKctA1dQNpXt5u_vXlYt5Qy-46Tdu3OthsfvN1uEjgC0Wp-7_1KY6F7Sl0tRxISebuhcQc76v67k-m0t7oYkolQ00ksfV1xanIxxCEcT9LQokIOVhW7LFid38VAAqXJvobIiWJpfrbRj_9ToDSPR3A49iNlQ0IdKqbBiQNPNU4zKYZEdLZshg1pMorklDNRncHRApahH7hGqP911lyahmKD7PRWfGTh0siL_uylyeM4ekYVzKN_KhQ7Ccu1t6aQ2NGl6VqHsr9EpnjGOh1O547VieHw8wur_0EX?type=png\"\n",
        "cosin_similarity_graph = \"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*dyH20eCqb6qTL-gt4nCVzQ.png\"\n",
        "\n",
        "# Interface Gradio\n",
        "demo = gr.Blocks()\n",
        "\n",
        "with demo:\n",
        "    gr.Markdown(\"# Démo IDP - OCR (ici Azure Vision) - Classification par similarité\")\n",
        "    gr.Image(value=diagram_url, label=\"\")\n",
        "\n",
        "    threshold_slider = gr.Slider(0.0, 1, value=0.5, label=\"Seuil similarité\")\n",
        "\n",
        "    outputs=[gr.Image(label=\"Image du doc\", type=\"pil\"), #gr.Textbox(label=\"Langue détectée\"),\n",
        "            gr.Textbox(label=\"Meilleur seuil\"),\n",
        "            gr.Json(label=\"Classification par similarité (0-Shot classification)\"),\n",
        "            gr.Textbox(label=\"Tous les mots extraits (et filtrés)\"),\n",
        "            gr.Json(label=\"Tags extraits\"),\n",
        "            gr.Json(label=\"Regroupement des sous-types par types (actuellement en mémoire)\")]\n",
        "\n",
        "    gr.Interface(\n",
        "            fn=process_document,\n",
        "            inputs=[gr.File(label=\"Fichier\", type=\"filepath\", file_types=[\".jpg\",\".pdf\",\".png\",\".docx\"]),\n",
        "                    gr.Textbox(label=\"Ajouter un sous-type de document\"),\n",
        "                    gr.Dropdown(\n",
        "                        [\"Preuve de scolarité\", \"Bulletin de notes\",\"Billet du médecin\", \"Spécimen de chèque\"], label=\"Dans la catégorie\", value=\"Preuve de scolarité\"),\n",
        "                    gr.CheckboxGroup([\"Nettoyage du texte\", \"Réinitialiser la liste des sous-types/types\", \"Calcul meilleur seuil\"], label=\"Paramètres\"),\n",
        "                    threshold_slider],\n",
        "            outputs=outputs,\n",
        "            title=None,\n",
        "            examples= [[\"fac_ortho.jpg\"],[\"billet_medecin.png\"],[\"recu_masso_photo.png\"],[\"McGill_Enrolment_Letter_2023-2024.pdf\"],[\"releve_notes.pdf\"]],\n",
        "            cache_examples=False, allow_flagging='never', clear_btn=None)\n",
        "\n",
        "    gr.Image(value=cosin_similarity_graph, label=\"\")"
      ],
      "metadata": {
        "id": "s54GdWR3jQcL"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "kJpgmOyiksgo",
        "outputId": "cb55ce0b-53bc-425a-e4bc-7ddb4d6fd61d"
      },
      "execution_count": 63,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://27776fc9d422f4399e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://27776fc9d422f4399e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://27776fc9d422f4399e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interface UI pour client"
      ],
      "metadata": {
        "id": "nOYTMK9hw5Pn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_document(img_path, seuil_max=0.49):\n",
        "    global sous_types_doc_original, type_mapping_original, sous_types_doc, type_mapping\n",
        "\n",
        "    img_path = convert_pdf_to_jpg(img_path)\n",
        "    img_path = convert_docx_to_pdf(img_path)\n",
        "    texts, tags = document_texts_detection(img_path=img_path)\n",
        "    texts = \" \".join(texts)\n",
        "    image = get_PIL_image(img_path=img_path)\n",
        "\n",
        "    texts = filter_text(texts)\n",
        "\n",
        "    types_doc_etendu = sous_types_doc_original.copy()\n",
        "    seuil, best = find_best_seuil(texts, types_doc_etendu, seuil_max=seuil_max, tol=1e-3)\n",
        "\n",
        "    analyse, best = classify_text(texts, types_doc_etendu, seuil=seuil)\n",
        "\n",
        "    return json.loads(analyse)[0][0], image"
      ],
      "metadata": {
        "id": "fMOg9m87xW9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Interface Gradio\n",
        "poc = gr.Blocks()\n",
        "\n",
        "with poc:\n",
        "    gr.Markdown(\"# PoC IDP - Classification de documents par similarité\")\n",
        "\n",
        "    outputs=[\n",
        "            gr.Text(label=\"Classification par similarité (0-Shot classification)\"),\n",
        "            gr.Image(label=\"Image du doc\", type=\"pil\")\n",
        "            ]\n",
        "\n",
        "    gr.Interface(\n",
        "            fn=process_document,\n",
        "            inputs=[gr.File(label=\"Fichier\", type=\"filepath\", file_types=[\".jpg\",\".pdf\",\".png\",\".docx\"]),\n",
        "                    ],\n",
        "            outputs=outputs,\n",
        "            title=None,\n",
        "            examples= [[\"billet_medecin.png\"],[\"McGill_Enrolment_Letter_2023-2024.pdf\"],[\"releve_notes.pdf\"]],\n",
        "            cache_examples=False, allow_flagging='never', clear_btn=None, submit_btn=\"Classifier\")\n"
      ],
      "metadata": {
        "id": "M0LennZixa1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poc.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "ZeFDLAaFxkOG",
        "outputId": "24790f07-ab5d-4437-dfa4-651324d2a3b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://3f40116f2a722632dc.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3f40116f2a722632dc.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://3f40116f2a722632dc.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##interface UI pour traitement en lot"
      ],
      "metadata": {
        "id": "9qF_hxAJRNEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_class(img_path, seuil_max=0.49):\n",
        "    global sous_types_doc_original, type_mapping_original, sous_types_doc, type_mapping\n",
        "\n",
        "    img_path = convert_pdf_to_jpg(img_path)\n",
        "    img_path = convert_docx_to_pdf(img_path)\n",
        "    texts, tags = document_texts_detection(img_path=img_path)\n",
        "    texts = \" \".join(texts)\n",
        "    image = get_PIL_image(img_path=img_path)\n",
        "\n",
        "    texts = filter_text(texts)\n",
        "\n",
        "    types_doc_etendu = sous_types_doc_original.copy()\n",
        "    seuil, best = find_best_seuil(texts, types_doc_etendu, seuil_max=seuil_max, tol=1e-3)\n",
        "\n",
        "    analyse, best = classify_text(texts, types_doc_etendu, seuil=seuil)\n",
        "\n",
        "    return json.loads(analyse)[0][0], seuil"
      ],
      "metadata": {
        "id": "fWLD_jLI5UA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def unzip_and_get_df(zip_file, batch_path=\"temp_extracted_files\", target_file=\"0-shot-classif.xlsx\"):\n",
        "    # Create a temporary directory to extract files\n",
        "    os.makedirs(batch_path, exist_ok=True)\n",
        "\n",
        "    # Extract the zip file\n",
        "    with zipfile.ZipFile(zip_file.name, 'r') as zip_ref:\n",
        "        zip_ref.extractall(batch_path)\n",
        "\n",
        "    # Locate the specific Excel file\n",
        "    file_path = os.path.join(batch_path, target_file)\n",
        "\n",
        "    # Check if the target file exists\n",
        "    if os.path.exists(file_path):\n",
        "        # Load the Excel file into a DataFrame\n",
        "        df = pd.read_excel(file_path)\n",
        "        return df\n",
        "    else:\n",
        "        raise FileNotFoundError(f\"File {target_file} not found in the zip archive.\")\n",
        "\n",
        "def highlight(s):\n",
        "    if s.target != s.predict:\n",
        "        if (s.target == \"Bulletin de notes\" and s.predict == \"Preuve de scolarité\") or (s.target == \"Preuve de scolarité\" and s.predict == \"Bulletin de notes\"):\n",
        "          return ['color:black;background-color:gray'] * len(s)\n",
        "        else:\n",
        "          return ['color:white;background-color:purple'] * len(s)\n",
        "    else:\n",
        "        return ['color:black;background-color:white'] * len(s)\n",
        "\n",
        "def process_data(zip_file, seuil_max, batch_path=\"temp_extracted_files\", target_file=\"0-shot-classif.xlsx\"):\n",
        "\n",
        "    try:\n",
        "      # Get the dataframe from the unzip_and_get_df function\n",
        "      df = unzip_and_get_df(zip_file, batch_path, target_file)\n",
        "\n",
        "      # Combine 'file_name' and 'file_ext' columns to create the list of files\n",
        "      df['doc_name'] = df['file_name'].astype(str) + '.' + df['file_ext'].astype(str)\n",
        "    except Exception as e:\n",
        "      txt_error = f\"Error processing zip file: {e}\"\n",
        "      print(txt_error)\n",
        "      return pd.DataFrame({'Erreur' : [txt_error]}), txt_error\n",
        "\n",
        "    # Process each file with process_document and update the dataframe\n",
        "    predicted_classes = []\n",
        "    best_seuils = []\n",
        "    for file_name in df['doc_name']:\n",
        "        file_path = os.path.join(batch_path, file_name)\n",
        "        try:\n",
        "          predicted_class, best_seuil = predict_class(file_path, seuil_max=seuil_max)\n",
        "          predicted_classes.append(predicted_class)\n",
        "          best_seuils.append(round(best_seuil,3))\n",
        "        except Exception as e:\n",
        "          print(f\"Error processing file {file_name}: {e}\")\n",
        "          predicted_classes.append(\"Erreur\")\n",
        "\n",
        "    df['predicted_class'] = predicted_classes\n",
        "    df['seuil'] = best_seuils\n",
        "    df['seuil'] = df['seuil'].map('{:,.2f}'.format)\n",
        "    df['is_correct_class'] = (df['predicted_class'] == df['target_class']).astype(int)\n",
        "\n",
        "    # Select only the required columns\n",
        "    df_result = df[['doc_name', 'target_class', 'predicted_class', 'seuil']]\n",
        "    df_result.columns = ['doc_name', 'target', 'predict', 'seuil']\n",
        "\n",
        "    # Calculate classification accuracy\n",
        "    overall_accuracy = df['is_correct_class'].mean() * 100\n",
        "    class_accuracy = df.groupby('target_class')['is_correct_class'].mean() * 100\n",
        "\n",
        "    textstr = f'Taux de bonne classification au global : {overall_accuracy:.2f}%\\n'\n",
        "    for cls, acc in class_accuracy.items():\n",
        "        textstr += f'         - Taux pour {cls}: {acc:.2f}%\\n'\n",
        "\n",
        "    return df_result.style.apply(highlight, axis=1), textstr\n",
        "\n",
        "poc = gr.Blocks()\n",
        "\n",
        "with poc:\n",
        "    gr.Markdown(\"# PoC IDP - Classification de documents par similarité - traitement en lot\")\n",
        "\n",
        "    input_zip = gr.File(label=\"Fichier ZIP contenant les échantillons à classer\")\n",
        "    threshold_slider = gr.Slider(0.0, 1, value=0.31, label=\"Seuil similarité max\")\n",
        "    submit_btn = gr.Button(\"Classifier\")\n",
        "\n",
        "    output_df = gr.Dataframe(label=\"\")\n",
        "    output_text = gr.Text(label=\"\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_data,\n",
        "        inputs=[input_zip, threshold_slider],\n",
        "        outputs=[output_df, output_text]\n",
        "    )\n"
      ],
      "metadata": {
        "id": "g490F9csRU8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poc.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "yhuqbekmnWva",
        "outputId": "cb41abae-5b39-4f6f-bed5-b7dbb1d8f830"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://508725e00ebb5bb049.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://508725e00ebb5bb049.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://508725e00ebb5bb049.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf temp_extracted_files"
      ],
      "metadata": {
        "id": "_0X8lEnEVuiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r 0-shot-classif.zip temp_extracted_files"
      ],
      "metadata": {
        "id": "8CEW9jHxGM8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}